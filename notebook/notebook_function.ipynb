{"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e78dbac0-88bf-4406-b505-b014169bee83"},{"cell_type":"code","source":["import yaml\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from notebookutils import mssparkutils\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"2c9ac686-41f1-42e9-b58e-2473124c1b23","normalized_state":"finished","queued_time":"2025-02-19T14:26:15.4296716Z","session_start_time":"2025-02-19T14:26:15.4310376Z","execution_start_time":"2025-02-19T14:26:29.2267071Z","execution_finish_time":"2025-02-19T14:26:31.9832337Z","parent_msg_id":"6b533579-62fa-4b87-80ff-1637f45d937e"},"text/plain":"StatementMeta(, 2c9ac686-41f1-42e9-b58e-2473124c1b23, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6d52951-da93-422e-944d-98db6e53aed6"},{"cell_type":"code","source":["def create_table(config, layer, lakehouse):\n","\n","    for table in config[\"tables\"]:\n","        if table[\"layer\"] == layer:  \n","\n","            schema_fields = [\n","                StructField(col_name, eval(col_def[\"type\"]), bool(col_def[\"nullable\"]))\n","                for col_name, col_def in table[\"schema\"].items()\n","            ]\n","            schema = StructType(schema_fields)  \n","\n","            empty_df = spark.createDataFrame([], schema)  \n","\n","            destination_path = f\"abfss://test@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{table['name']}\"\n","            empty_df.write.format(\"delta\").mode(\"overwrite\").save(destination_path)\n","            #ajout table rejet\n","            if layer == \"bronze\":\n","                destination_path_reject = f\"abfss://test@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{table['name']}_reject\"\n","                empty_df.write.format(\"delta\").mode(\"overwrite\").save(destination_path_reject)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"b4fb66bd-95f8-4474-b049-b920b8f03895","normalized_state":"finished","queued_time":"2025-02-26T10:05:45.8032072Z","session_start_time":"2025-02-26T10:05:45.804614Z","execution_start_time":"2025-02-26T10:05:58.9766475Z","execution_finish_time":"2025-02-26T10:06:01.1994352Z","parent_msg_id":"974ce13c-6f0a-4c65-ad30-7f761f54abbd"},"text/plain":"StatementMeta(, b4fb66bd-95f8-4474-b049-b920b8f03895, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"606fa567-0952-4b0f-9369-d298b1ace649"},{"cell_type":"code","source":["def load_data(config, lakehouse, file_format, notebook_name, sources_to_process):\n","    for table in config[\"tables\"]:\n","        if \"bronze\" not in table:\n","            continue  \n","\n","        table_name = table[\"name\"]\n","        source_path = table[\"bronze\"][\"source_path\"]\n","\n","        if source_path not in sources_to_process:\n","            continue\n","\n","        print(f\"\\nüöÄ Traitement du dossier source : {source_path} pour la table {table_name}\")\n","\n","        file_path_base = f\"abfss://test@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/{source_path}\"\n","\n","        table_bronze = f\"{lakehouse}.{table_name}\"\n","        table_reject = f\"{lakehouse}.{table_name}_reject\"\n","\n","        job_start = datetime.now()\n","        job_id = int(uuid.uuid4().int % (10**10))\n","        valid_files = 0\n","        invalid_files = 0\n","        total_files = 0\n","        job_status = \"INIT\"\n","        job_message = \"\"\n","\n","        try:\n","            files_list = mssparkutils.fs.ls(file_path_base)\n","            total_files = len(files_list) if files_list else 0\n","\n","            for file in files_list:\n","                file_name = file.name\n","                file_path = f\"{file_path_base}/{file_name}\"\n","                flow_id = int(uuid.uuid4().int % (10**10))\n","\n","                print(f\"üìÇ Lecture du fichier : {file_name}\")\n","\n","                if file_format == \"csv\":\n","                    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").load(file_path)\n","                elif file_format == \"parquet\":\n","                    df = spark.read.format(\"parquet\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_path)\n","                else:\n","                    raise Exception(f\"Format de fichier non support√© : {file_format}\")\n","\n","                print(f\"‚úÖ {df.count()} lignes charg√©es depuis {file_name}\")\n","\n","                primary_keys = [col[\"source_name\"] for col in table[\"silver\"][\"columns\"] if col[\"is_nullable\"] == 0]\n","\n","                df_valid = None\n","                df_reject = None\n","                df_valid_count = 0\n","                df_reject_count = 0\n","\n","                if primary_keys:\n","                    df_valid = df.dropna(subset=primary_keys)\n","                    df_valid_count = df_valid.count()\n","                    df_reject = df.subtract(df_valid)\n","                    df_reject_count = df_reject.count()\n","                else:\n","                    df_valid = df\n","                    df_valid_count = df.count()\n","\n","                if df_valid_count > 0:\n","                    df_valid.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(table_bronze)\n","                    valid_files += 1\n","                    print(f\"üíæ Donn√©es valides √©crites dans {table_bronze}\")\n","\n","                if df_reject_count > 0:\n","                    df_reject.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(table_reject)\n","                    invalid_files += 1\n","                    print(f\"‚ö†Ô∏è Donn√©es rejet√©es √©crites dans {table_reject}\")\n","\n","                job_status = \"SUCCESS\"\n","                job_message = \"Fichier trait√© avec succ√®s\"\n","\n","                flow_data = {\n","                    \"job_id\": job_id,\n","                    \"flow_id\": flow_id,\n","                    \"file_path\": file_path,\n","                    \"flow_start\": job_start,\n","                    \"flow_end\": datetime.now(),\n","                    \"flow_status\": job_status,\n","                    \"flow_message\": job_message,\n","                    \"accepted_rows\": df_valid_count,\n","                    \"warning_rows\": 0,\n","                    \"rejected_rows\": df_reject_count,\n","                    \"total_rows\": df_valid_count + df_reject_count,\n","                    \"year\": int(job_start.year),\n","                    \"month\": int(job_start.month),\n","                    \"day\": int(job_start.day)\n","                }\n","\n","                log_flow(flow_data)\n","                print(f\"üìù Log flow ajout√© pour {file_name} - Flow ID : {flow_id}\")\n","\n","        except Exception as e:\n","            job_status = \"FAILED\"\n","            job_message = str(e)\n","            print(f\"‚ùå Erreur lors du traitement de {table_name} : {job_message}\")\n","\n","        # Log final du job d‚Äôingestion\n","        log_data = {\n","            \"job_id\": job_id,\n","            \"orchestration_id\": 0,\n","            \"job_name\": \"load_data\",\n","            \"job_type\": \"ingestion\",\n","            \"notebook_name\": notebook_name,\n","            \"source_name\": source_path,\n","            \"object_name\": \"\",\n","            \"job_start\": job_start,\n","            \"job_end\": datetime.now(),\n","            \"valid_files\": valid_files,\n","            \"invalid_files\": invalid_files,\n","            \"total_files\": total_files,\n","            \"source_layer\": \"raw\",\n","            \"target_layer\": \"bronze\",\n","            \"target_table_name\": table_name,\n","            \"job_status\": job_status,\n","            \"job_message\": job_message,\n","            \"year\": int(job_start.year),\n","            \"month\": int(job_start.month),\n","            \"day\": int(job_start.day)\n","        }\n","\n","        log_job_execution(log_data)\n","        print(f\"üìå Log job ajout√© pour {table_name} - Statut : {job_status}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"e6ec0f7d-e199-4f43-b843-ef4e632d18ab","normalized_state":"finished","queued_time":"2025-03-21T09:45:47.2820483Z","session_start_time":"2025-03-21T09:45:47.283884Z","execution_start_time":"2025-03-21T09:45:59.578903Z","execution_finish_time":"2025-03-21T09:45:59.9623964Z","parent_msg_id":"7f9e4bfa-040c-4691-b899-376a6928b69d"},"text/plain":"StatementMeta(, e6ec0f7d-e199-4f43-b843-ef4e632d18ab, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df231ac7-0391-4122-adfa-4f590711f8ec"},{"cell_type":"code","source":["def insert_bronze_to_silver(config, mode, notebook_name):\n","\n","    for table in config[\"tables\"]:\n","        if \"silver\" not in table or \"bronze\" not in table:\n","            continue  \n","\n","        bronze_table = f\"lakehouse_bronze.{table['name']}\"\n","        silver_table = f\"lakehouse_silver.{table['name']}\"\n","        job_start = datetime.now()\n","        job_id = int(uuid.uuid4().int % (10**10))  \n","        flow_id = int(uuid.uuid4().int % (10**10))\n","\n","        df_valid = None\n","        df_reject = None\n","        df_valid_count = 0\n","        df_reject_count = 0\n","        table_name = table[\"name\"]\n","\n","\n","\n","        try:\n","            \n","            df_bronze = spark.read.format(\"delta\").load(f\"abfss://test@onelake.dfs.fabric.microsoft.com/lakehouse_bronze.Lakehouse/Tables/{table['name']}\")\n","\n","            \n","            transformation_dict = {col[\"source_name\"]: col[\"target_name\"] for col in table[\"silver\"][\"columns\"]}\n","            df_silver = df_bronze.select([col(c).alias(transformation_dict[c]) for c in transformation_dict])\n","\n","            print(f\" Colonnes transform√©es : {df_silver.columns}\")\n","\n","        \n","            if mode == \"SCD0\":\n","                df_silver.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(silver_table)\n","                job_status, job_message = \"SUCCESS\", \"Ajout des nouvelles donn√©es\"\n","                print(f\" Mode SCD0 :{job_message}\")\n","\n","            elif mode == \"SCD1\":\n","                df_silver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(silver_table)\n","                job_status, job_message = \"SUCCESS\", \"√âcrasement des anciennes donn√©es\"\n","                print(f\"‚úÖ Mode SCD1 : {job_message}\")\n","\n","        except Exception as e:\n","            job_status, job_message = \"FAILED\", str(e)\n","            print(f\"{job_message}\")\n","\n","            df_valid_count = df_valid.count() if df_valid is not None else 0\n","            df_reject_count = df_reject.count() if df_reject is not None else 0\n","\n","      \n","        log_data = {\n","            \"job_id\": job_id,  \n","            \"orchestration_id\": 0,  \n","            \"job_name\": \"insert_bronze_to_silver\",\n","            \"job_type\": \"ingestion\",\n","            \"notebook_name\": notebook_name,\n","            \"source_name\": bronze_table,\n","            \"object_name\": \"\",\n","            \"job_start\": job_start,\n","            \"job_end\": datetime.now(),\n","            \"valid_files\": 0, \n","            \"invalid_files\": 0,  \n","            \"total_files\": 0,  \n","            \"source_layer\": \"bronze\",\n","            \"target_layer\": \"silver\",\n","            \"target_table_name\": table[\"name\"],\n","            \"job_status\": job_status,\n","            \"job_message\": job_message,\n","            \"year\": int(job_start.year),\n","            \"month\": int(job_start.month),\n","            \"day\": int(job_start.day)\n","        }\n","\n","        log_job_execution(log_data)  \n","        \n","        flow_data = {\n","            \"job_id\": job_id,\n","            \"flow_id\": flow_id,\n","            \"file_path\":f\"abfss://test@onelake.dfs.fabric.microsoft.com/lakehouse_bronze.Lakehouse/Tables/{table['name']}\",\n","            \"flow_start\":job_start,\n","            \"flow_end\":datetime.now(),\n","            \"flow_status\":job_status,\n","            \"flow_message\":job_message,\n","            \"accepted_rows\":df_valid_count,\n","            \"warning_rows\":0,\n","            \"rejected_rows\":df_reject_count,\n","            \"total_rows\":df_valid_count + df_reject_count,\n","            \"year\": int(job_start.year),\n","            \"month\": int(job_start.month),\n","            \"day\": int(job_start.day)\n","        }\n","\n","        log_flow(flow_data)\n","        print(f\"log flow ajout√© avec succ√®s pour {table_name}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"761efd84-b69d-4c2b-beb5-8993450bbbce","normalized_state":"finished","queued_time":"2025-03-21T11:52:49.187808Z","session_start_time":"2025-03-21T11:52:49.1894514Z","execution_start_time":"2025-03-21T11:52:59.1893539Z","execution_finish_time":"2025-03-21T11:52:59.5284103Z","parent_msg_id":"ab0c00e5-42c0-4c04-af8d-37c3fe4441cb"},"text/plain":"StatementMeta(, 761efd84-b69d-4c2b-beb5-8993450bbbce, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a193b80a-1e1f-4ebc-b333-3e756cc816ff"},{"cell_type":"code","source":["def insert_silver_to_gold():\n","   \n","    df_silver_employee = spark.read.table(\"lakehouse_silver.employee\")\n","    df_silver_performance = spark.read.table(\"lakehouse_silver.performance_rating\")\n","    df_silver_satisfaction = spark.read.table(\"lakehouse_silver.satisfied_level\")\n","    df_silver_rating = spark.read.table(\"lakehouse_silver.rating_level\")\n","    df_silver_education = spark.read.table(\"lakehouse_silver.education_level\")\n","\n","    df_gold_employee = (\n","        df_silver_employee.join(df_silver_education, df_silver_employee[\"education\"] == df_silver_education[\"education_level_id\"], \"left\").join(df_silver_performance, \"employee_id\", \"left\").select(\n","            df_silver_employee[\"employee_id\"],\n","            df_silver_employee[\"first_name\"],\n","            df_silver_employee[\"last_name\"],\n","            df_silver_employee[\"gender\"],\n","            df_silver_employee[\"age\"],\n","            df_silver_employee[\"department\"],\n","            df_silver_employee[\"job_role\"],\n","            df_silver_education[\"education_level\"],\n","            df_silver_employee[\"salary\"],\n","            df_silver_employee[\"attrition\"],\n","            df_silver_performance[\"job_satisfaction\"],\n","            df_silver_performance[\"manager_rating\"],\n","            df_silver_performance[\"work_life_balance\"]\n","        )\n","    )\n","\n","    df_gold_employee.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_gold.gold_employee\")\n","    print(\" Donn√©es ins√©r√©es dans `gold_employee`.\")\n","\n","    df_gold_performance = (\n","        df_silver_performance.join(df_silver_employee, \"employee_id\", \"left\").join(df_silver_satisfaction, df_silver_performance[\"job_satisfaction\"] == df_silver_satisfaction[\"satisfaction_id\"], \"left\").join(df_silver_rating, df_silver_performance[\"self_rating\"] == df_silver_rating[\"rating_id\"], \"left\").select(\n","            df_silver_performance[\"performance_id\"],\n","            df_silver_performance[\"employee_id\"],\n","            df_silver_performance[\"review_date\"],\n","            df_silver_employee[\"job_role\"],\n","            df_silver_employee[\"salary\"],\n","            df_silver_performance[\"job_satisfaction\"],\n","            df_silver_satisfaction[\"satisfaction_level\"],\n","            df_silver_performance[\"self_rating\"],\n","            df_silver_rating[\"rating_level\"],\n","            df_silver_performance[\"manager_rating\"],\n","            df_silver_performance[\"training_opportunities_within_year\"],\n","            df_silver_performance[\"work_life_balance\"]\n","        )\n","    )\n","\n","    df_gold_performance.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_gold.gold_performance_rating\")\n","    print(\" Donn√©es ins√©r√©es dans `gold_performance_rating`.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"216943ad-87f8-40b5-813e-7b50aa5a9892","normalized_state":"finished","queued_time":"2025-03-03T15:17:31.3902778Z","session_start_time":"2025-03-03T15:17:31.3916366Z","execution_start_time":"2025-03-03T15:17:45.7919491Z","execution_finish_time":"2025-03-03T15:17:47.9085228Z","parent_msg_id":"7e65d235-752d-4c7a-a0a3-6d001c86a776"},"text/plain":"StatementMeta(, 216943ad-87f8-40b5-813e-7b50aa5a9892, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b808dc4e-cd8c-41b1-9032-47cd6ae22ba9"},{"cell_type":"code","source":["def log_job_execution(log_data):\n","    workspace_name = mssparkutils.env.getWorkspaceName()\n","    monitoring_table = f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/monitoring.Lakehouse/Tables/job_execution\"\n","\n","    schema = StructType([\n","        StructField(\"job_id\", LongType(), True),\n","        StructField(\"orchestration_id\", LongType(), True),\n","        StructField(\"job_name\", StringType(), True),\n","        StructField(\"job_type\", StringType(), True),\n","        StructField(\"notebook_name\", StringType(), True),\n","        StructField(\"source_name\", StringType(), True),\n","        StructField(\"object_name\", StringType(), True),\n","        StructField(\"job_start\", TimestampType(), True),\n","        StructField(\"job_end\", TimestampType(), True),\n","        StructField(\"valid_files\", IntegerType(), True),\n","        StructField(\"invalid_files\", IntegerType(), True),\n","        StructField(\"total_files\", IntegerType(), True),\n","        StructField(\"source_layer\", StringType(), True),\n","        StructField(\"target_layer\", StringType(), True),\n","        StructField(\"target_table_name\", StringType(), True),\n","        StructField(\"job_status\", StringType(), True),\n","        StructField(\"job_message\", StringType(), True),\n","        StructField(\"year\", ShortType(), True),\n","        StructField(\"month\", ShortType(), True),\n","        StructField(\"day\", ShortType(), True)\n","    ])\n","\n","    try:\n","        df_monitoring = spark.createDataFrame([log_data], schema=schema)\n","        df_monitoring.write.format(\"delta\").mode(\"append\").save(monitoring_table)\n","\n","        print(f\" Log ajout√© avec succ√®s pour {log_data['target_table_name']}\")\n","\n","    except Exception as e:\n","        print(f\" Erreur lors de l'insertion du log : {str(e)}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"2e4a4595-ad9e-4a9d-9da2-94a8741eac23","normalized_state":"finished","queued_time":"2025-02-24T09:04:05.974785Z","session_start_time":null,"execution_start_time":"2025-02-24T09:04:06.1244262Z","execution_finish_time":"2025-02-24T09:04:06.3520431Z","parent_msg_id":"d675bfb2-79a4-454e-99e7-231ce955fa36"},"text/plain":"StatementMeta(, 2e4a4595-ad9e-4a9d-9da2-94a8741eac23, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d07daac5-f7a3-47ac-b316-958f16771b93"},{"cell_type":"code","source":["def log_flow(log_flow_data):\n","    workspace_name = mssparkutils.env.getWorkspaceName()\n","    monitoring_table = f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/monitoring.Lakehouse/Tables/flow\"\n","\n","    schema = StructType([\n","        StructField(\"job_id\", LongType(), True),\n","        StructField(\"flow_id\", LongType(), True),\n","        StructField(\"file_path\", StringType(), True),\n","        StructField(\"flow_start\", TimestampType(), True),\n","        StructField(\"flow_end\", TimestampType(), True),\n","        StructField(\"flow_status\", StringType(), True),\n","        StructField(\"flow_message\", StringType(), True),\n","        StructField(\"accepted_rows\", IntegerType(), True),\n","        StructField(\"warning_rows\", IntegerType(), True),\n","        StructField(\"rejected_rows\", IntegerType(), True),\n","        StructField(\"total_rows\", IntegerType(), True),\n","        StructField(\"year\", ShortType(), True),\n","        StructField(\"month\", ShortType(), True),\n","        StructField(\"day\", ShortType(), True)\n","    ])\n","\n","    try:\n","        \n","        df_flow = spark.createDataFrame([log_flow_data], schema=schema)\n","        df_flow.write.format(\"delta\").mode(\"append\").save(monitoring_table)\n","\n","        print(f\"‚úÖ Log ajout√© avec succ√®s pour {log_flow_data['file_path']}\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Erreur lors de l'insertion du log dans `flow` : {str(e)}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"1b82d624-f09b-4e90-95bb-77c8d0121286","normalized_state":"finished","queued_time":"2025-02-27T10:04:13.7430069Z","session_start_time":"2025-02-27T10:04:13.7443135Z","execution_start_time":"2025-02-27T10:04:27.6399936Z","execution_finish_time":"2025-02-27T10:04:30.0193535Z","parent_msg_id":"4e6b315a-599f-4ba5-9836-a63203fd3e20"},"text/plain":"StatementMeta(, 1b82d624-f09b-4e90-95bb-77c8d0121286, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1755aab8-02aa-41f9-a5b8-00d70d2adb01"},{"cell_type":"code","source":["def log_orchestration(log_data):\n","    workspace_name = mssparkutils.env.getWorkspaceName()\n","    monitoring_table = f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/monitoring.Lakehouse/Tables/orchestration\"\n","\n","    schema = StructType([\n","        StructField(\"orchestration_id\", LongType(), True),\n","        StructField(\"suplementary_run_id\", StringType(), True),\n","        StructField(\"orchestration_name\", StringType(), True),\n","        StructField(\"data_product_name\", StringType(), True),\n","        StructField(\"domain_name\", StringType(), True),\n","        StructField(\"entity_name\", StringType(), True),\n","        StructField(\"environment\", StringType(), True),\n","        StructField(\"workspace_name\", StringType(), True),\n","        StructField(\"user_name\", StringType(), True),\n","        StructField(\"orchestration_start\", TimestampType(), True),\n","        StructField(\"orchestration_end\", TimestampType(), True),\n","        StructField(\"orchestration_status\", StringType(), True),\n","        StructField(\"orchestration_message\", StringType(), True),\n","        StructField(\"year\", ShortType(), True),\n","        StructField(\"month\", ShortType(), True),\n","        StructField(\"day\", ShortType(), True)\n","    ])\n","\n","    try:\n","        df_log = spark.createDataFrame([log_data], schema=schema)\n","        df_log.write.format(\"delta\").mode(\"append\").save(monitoring_table)\n","        print(f\"üìå Log orchestration ajout√© avec statut : {log_data['orchestration_status']}\")\n","    except Exception as e:\n","        print(f\"‚ùå Erreur lors de l'insertion du log dans `orchestration` : {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"91d2663e-08bb-4673-8e67-2a436926517d","normalized_state":"finished","queued_time":"2025-03-21T14:12:40.3395044Z","session_start_time":"2025-03-21T14:12:40.3408734Z","execution_start_time":"2025-03-21T14:12:53.9879381Z","execution_finish_time":"2025-03-21T14:12:54.443641Z","parent_msg_id":"99bd24f3-db7b-417e-8dff-d1bf0dc63256"},"text/plain":"StatementMeta(, 91d2663e-08bb-4673-8e67-2a436926517d, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"edb1e517-6fff-454d-8919-a74df1caa0cf"},{"cell_type":"code","source":["def extract_sources_from_yaml(config, yaml_key=\"tables_gold\"):\n","    orchestration_start = datetime.now()\n","    user_name = getpass.getuser()\n","    workspace_name = mssparkutils.env.getWorkspaceName()\n","    environment = \"dev\"\n","    data_product_name = \"load_employee_data\"\n","    domain_name = \"HR\"\n","    entity_name = \"employee\"\n","    orchestration_name = \"extract_sources\"\n","    orchestration_id = int(uuid.uuid4().int % (10**10))\n","    suplementary_run_id = str(uuid.uuid4())\n","    sources = []\n","\n","    try:\n","        if yaml_key in config and config[yaml_key]:\n","            for table in config[yaml_key]:\n","                if \"sources\" in table:\n","                    for source in table[\"sources\"]:\n","                        if \"source_path\" in source:\n","                            sources.append(source[\"source_path\"])\n","            orchestration_status = \"SUCCESS\"\n","            orchestration_message = f\"{len(sources)} sources extraites avec succ√®s\"\n","        else:\n","            orchestration_status = \"FAILED\"\n","            orchestration_message = f\"La cl√© '{yaml_key}' est absente ou vide dans le YAML\"\n","    except Exception as e:\n","        orchestration_status = \"FAILED\"\n","        orchestration_message = str(e)\n","\n","    orchestration_end = datetime.now()\n","\n","    log_data = {\n","        \"orchestration_id\": orchestration_id,\n","        \"suplementary_run_id\": suplementary_run_id,\n","        \"orchestration_name\": orchestration_name,\n","        \"data_product_name\": data_product_name,\n","        \"domain_name\": domain_name,\n","        \"entity_name\": entity_name,\n","        \"environment\": environment,\n","        \"workspace_name\": workspace_name,\n","        \"user_name\": user_name,\n","        \"orchestration_start\": orchestration_start,\n","        \"orchestration_end\": orchestration_end,\n","        \"orchestration_status\": orchestration_status,\n","        \"orchestration_message\": orchestration_message,\n","        \"year\": orchestration_start.year,\n","        \"month\": orchestration_start.month,\n","        \"day\": orchestration_start.day\n","    }\n","\n","    log_orchestration(log_data)\n","    return list(set(sources))\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"91d2663e-08bb-4673-8e67-2a436926517d","normalized_state":"finished","queued_time":"2025-03-21T14:14:01.3975085Z","session_start_time":null,"execution_start_time":"2025-03-21T14:14:01.3993789Z","execution_finish_time":"2025-03-21T14:14:01.6735254Z","parent_msg_id":"8b5b6af2-4f61-48ce-b8aa-2a7358fb8e4a"},"text/plain":"StatementMeta(, 91d2663e-08bb-4673-8e67-2a436926517d, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3d3042fb-050e-4eb4-b1bf-c268b32f95e8"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a9be8a2-bdd9-4e39-a21b-354fac29b419"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"fr"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}